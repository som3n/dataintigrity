# dataintegrity

**A production-grade Python SDK for data integrity, quality scoring, PII detection, statistical drift detection, and dataset versioning.**

> v0.2 â€” State-aware data integrity system with historical comparison engine

---

## Features

| Capability | Status |
|---|---|
| CSV & PostgreSQL connectors | âœ… |
| Large CSV safety (auto-sampling > 50 MB) | âœ… |
| Column normalisation | âœ… |
| Schema contract enforcement | âœ… |
| PII detection (email, phone, SSN, CC) | âœ… |
| Completeness, Uniqueness, Validity, Consistency, Timeliness rules | âœ… |
| Composite DataScore (weighted, 0â€“100) | âœ… |
| SHA-256 dataset fingerprint | âœ… |
| Statistical drift detection (KS test) | âœ… |
| **Dataset versioning (v0.2)** | âœ… |
| **Local baseline store (v0.2)** | âœ… |
| **Historical comparison engine (v0.2)** | âœ… |
| **Score delta + severity tracking (v0.2)** | âœ… |
| CLI audit command | âœ… |
| **CLI --track & --history flags (v0.2)** | âœ… |

---

## Installation

```bash
# Editable install (recommended for development)
pip install -e .

# Or standard install
pip install dataintegrity
```

---

## Quick Start â€” CLI (v0.2)

### Single audit
```bash
dataintegrity audit sample.csv
```

### Track versions and compare
```bash
# First run â€” establishes baseline
dataintegrity audit sample.csv --track

# Second run â€” compares against baseline
dataintegrity audit sample.csv --track
```

**Example `--track` output (second run):**
```
ðŸ’¾  Version saved  id=a3b4c5d6e7f80001

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  DELTA COMPARISON REPORT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Previous DataScore : 89.3
  Current  DataScore : 85.1
  Delta              : -4.2  (MODERATE)
  Baseline version   : f1e2d3c4b5a60001

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  DIMENSION DELTAS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  completeness     -3.50%
  uniqueness       -1.20%
  validity          0.00%
  consistency      -0.80%
  timeliness        0.00%

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  DRIFT DETECTION (KS TEST)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âš   Drift detected in:
      - transaction_amount
      - session_duration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Show version history
```bash
dataintegrity audit sample.csv --history
```

**Example output:**
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  VERSION HISTORY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  [ 1]  2026-02-22 10:00:00 UTC  score=89.3  id=f1e2d3c4b5a60001
  [ 2]  2026-02-22 10:15:00 UTC  score=85.1  id=a3b4c5d6e7f80001
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Combined flags
```bash
dataintegrity audit sample.csv --track --history
```

---

## Severity Scale

| Level | Score Drop |
|---|---|
| ðŸŸ¢ STABLE | < 2 points |
| ðŸ”µ MINOR | 2â€“5 points |
| ðŸŸ¡ MODERATE | 5â€“10 points |
| ðŸ”´ CRITICAL | â‰¥ 10 points |

---

## Version Storage

Versions are stored locally under `~/.dataintegrity/versions/`, one JSON file per data source:

```
~/.dataintegrity/versions/
    <sha256-of-source-path>.json   # all versions for one source
```

File structure:
```json
{
  "source": "/path/to/sample.csv",
  "versions": [
    {
      "version_id": "f1e2d3c4b5a60001",
      "timestamp": "2026-02-22T10:00:00+00:00",
      "fingerprint": "3f4a1b2c...",
      "data_score": 89.3,
      "dimension_scores": { "completeness": 0.95, "..." : "..." },
      "source": "/path/to/sample.csv"
    }
  ]
}
```

---

## Quick Start â€” Python SDK

### Basic audit
```python
from dataintegrity.connectors.csv import CSVConnector
from dataintegrity.core.dataset import Dataset
from dataintegrity.integrity.engine import IntegrityEngine

connector = CSVConnector("sample.csv")
connector.connect()
df = connector.fetch()

dataset = Dataset(df, source="sample.csv")
result = IntegrityEngine().run(dataset)
print(f"DataScore: {result['data_score']}")
```

### Versioning and comparison
```python
from dataintegrity.core.versioning import DatasetVersion
from dataintegrity.core.store import LocalVersionStore
from dataintegrity.integrity.comparison import IntegrityComparator

store = LocalVersionStore()

# After running the engine:
version = DatasetVersion(dataset)  # captures score + fingerprint
store.save(version)

# On next run:
previous = store.load_latest("sample.csv")
current = DatasetVersion(new_dataset)

comparator = IntegrityComparator()
report = comparator.compare_versions(current, previous)
print(report)
# {
#   "score_delta": -4.2,
#   "dimension_deltas": {"completeness": -0.035, ...},
#   "drifted_columns": ["transaction_amount", "session_duration"],
#   "severity": "moderate",
#   ...
# }
```

### Large file safety
```python
# Auto-samples if > 50 MB (logged as WARNING)
connector = CSVConnector("huge.csv")
connector.connect()
df = connector.fetch()

# Explicit sampling
connector = CSVConnector("huge.csv", sample_size=50_000)

# Chunked streaming
connector = CSVConnector("huge.csv", chunk_size=10_000, sample_size=50_000)
```

---

## Package Structure (v0.2)

```
dataintegrity/
â”œâ”€â”€ connectors/
â”‚   â”œâ”€â”€ base.py             # Abstract connector
â”‚   â”œâ”€â”€ csv.py              # CSV connector (+ large-file safety)
â”‚   â””â”€â”€ postgres.py         # PostgreSQL connector
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ normalizer.py       # Column normalisation
â”‚   â”œâ”€â”€ pii.py              # PII detection
â”‚   â””â”€â”€ schema_contract.py  # Schema enforcement
â”œâ”€â”€ integrity/
â”‚   â”œâ”€â”€ rules.py            # Quality rules (5 dimensions)
â”‚   â”œâ”€â”€ scorer.py           # DataScorer (weighted aggregation)
â”‚   â”œâ”€â”€ engine.py           # IntegrityEngine (orchestration)
â”‚   â””â”€â”€ comparison.py       # IntegrityComparator (NEW v0.2)
â”œâ”€â”€ drift/
â”‚   â””â”€â”€ ks.py               # KS-test drift detection
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py           # IntegrityConfig
â”‚   â”œâ”€â”€ dataset.py          # Dataset abstraction
â”‚   â”œâ”€â”€ hashing.py          # SHA-256 fingerprinting
â”‚   â”œâ”€â”€ versioning.py       # DatasetVersion (NEW v0.2)
â”‚   â””â”€â”€ store.py            # LocalVersionStore (NEW v0.2)
â””â”€â”€ cli.py                  # CLI (+ --track, --history)
```

---

## Configuration

```python
from dataintegrity.core.config import IntegrityConfig

config = IntegrityConfig(
    drift_p_threshold=0.01,          # Stricter drift detection
    timeliness_max_age_days=7,       # Data must be < 7 days old
    score_weights={
        "completeness": 0.40,
        "uniqueness":   0.20,
        "validity":     0.20,
        "consistency":  0.10,
        "timeliness":   0.10,
    },
)
```

---

## All CLI Options

```
Usage: dataintegrity audit [OPTIONS] FILEPATH

Options:
  --encoding TEXT        CSV file encoding.  [default: utf-8-sig]
  --delimiter TEXT       CSV column delimiter.  [default: ,]
  --no-normalize         Skip column-name normalisation.
  --json-output          Print results as JSON instead of formatted text.
  --pii-threshold FLOAT  Drift p-value threshold.  [default: 0.05]
  --sample-size INT      Max rows to read (large CSV safety).
  --track                Save version and compare to previous.
  --history              Show version history for this source.
  --help                 Show this message and exit.
```

---

## Roadmap (post-v0.2)

- [ ] JSON / Parquet / S3 connectors
- [ ] API server (FastAPI)
- [ ] Monitoring dashboard
- [ ] Rule extensibility framework
- [ ] JWT-secured audit certification
- [ ] Slack / webhook alerting

---

## License

MIT
